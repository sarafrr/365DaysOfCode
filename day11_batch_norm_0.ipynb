{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalisation in Detail - First Lecture\n",
    "\n",
    "When we batch normalise a data (batch), we obtain a tensor of the same dimension of the input tensor.\n",
    "\n",
    "We have to specify to the function `nn.BatchNorm2d` the n. of features of our batch.\n",
    "In the case of a convolutional layer output, it is the second dimension of a `torch.tensor`.\n",
    "In the case of a linear layer output, it is the first dimension of the `torch.tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "norm = nn.BatchNorm2d(2,track_running_stats=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img.shape = torch.Size([3, 2, 1, 4])\n",
      "out.shape = torch.Size([3, 2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "elements = np.arange(24)\n",
    "n_features = 2\n",
    "img = torch.from_numpy(elements).view((3,n_features,1,4)).type(dtype=torch.float)\n",
    "\n",
    "out = norm(img)\n",
    "\n",
    "print(f'{img.shape = }')\n",
    "print(f'{out.shape = }')\n",
    "# out.shape has the same dimension of the input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show ho to compute correctly the mean and variance of a batch to be used in\n",
    "the formula for batch normalising.\n",
    "\n",
    "1. Compute one after the other the means considering all the other axes then the second one.\n",
    "In this way, we obtain the correct mean of the batch of dimension `(1,n_features,1,1)`.\n",
    "2. Compute the variance using this mean you've just computed. Thus using the explicit formula to\n",
    "compute the variance (through the use of the `torch.mean` function).\n",
    "\n",
    "*Note*: If you use the formula to compute the variance which is computed on one axis after the other,\n",
    "you will obtain a wrong result, because you would be using the mean of each axis for each operation.\n",
    "And, this is not the mean of the whole batch. It is not the same as using always the last computed mean.\n",
    "\n",
    "The parameter `keepdim` permits to retain the axis (which will be of dimension 1) where we compute the statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct way to do batch normalisation\n",
    "mean_channels = torch.mean(img,dim=[0,2,3], keepdim=True)\n",
    "# the operation on the mean is the same of:\n",
    "mean_axis0 = torch.mean(img,dim=[0], keepdim=True)\n",
    "mean_axis2 = torch.mean(mean_axis0,dim=[2], keepdim=True)\n",
    "mean_axis3 = torch.mean(mean_axis2,dim=[3], keepdim=True)\n",
    "# only way to compute the variance correctly\n",
    "var1_through_ch = torch.mean((img - mean_axis3)**2,dim=[0,2,3], keepdim=True)\n",
    "\n",
    "print(f'{mean_axis0.shape}')\n",
    "print(f'{mean_axis2.shape}')\n",
    "print(f'{mean_axis3.shape}')\n",
    "\n",
    "# this is to see if the mean computed in the two ways is the same\n",
    "print((mean_axis3.eq(mean_channels)).sum().item()==n_features)\n",
    "\n",
    "# this is not the correct way to compute the variance for batch normalisation\n",
    "var_through_ch = torch.var(img,dim=[0,2,3], keepdim=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following computation of the normalisation 2D for the batch, we use the default\n",
    "value as number for numerical stability `eps=1e-5`.\n",
    "The results are slight different for the batch normalised tensor using the already implemented\n",
    "function `nn.BatchNorm2D` and the computation from scratch.\n",
    "*We saw that the difference starts by the 7th decimal digit*, indeed by using as absolute tolerance in `torch.isclose` equal to `1e-6`, we have that\n",
    "all the elements of the two normalised data are the same.\n",
    "If want more digits to compare, as with `1e-7` as precision, we will have that not all the elements\n",
    "are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "eps = 1e-5\n",
    "abs_tol = 1e-6\n",
    "\n",
    "norm1_batch = (img-mean_channels)/torch.sqrt(var1_through_ch + eps)\n",
    "norm1_batch.requires_grad_(False)\n",
    "\n",
    "print(out)\n",
    "print(norm1_batch)\n",
    "\n",
    "# norm1_batch.eq(out)\n",
    "print((norm1_batch.isclose(out, atol=abs_tol, rtol=0).sum()==img.numel()).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp-coeff-learning-wRQaBgx8-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df1579a085a1741667bface338fb2721eaeea1f79cf4cabfbc018aa5e1dc1609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
