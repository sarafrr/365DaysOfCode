{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "We have seen that the dropout every time is applied uses different randomness\n",
    "to decide what elements to put to zero.\n",
    "Both in the case we use:\n",
    "1. the same layer to get the sparse output,  \n",
    "2. two different layers called once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn((1,10,100))\n",
    "\n",
    "drop_1 = nn.Dropout(0.2)\n",
    "drop_2 = nn.Dropout(0.2)\n",
    "out1_first = drop_1(x)\n",
    "out1_second = drop_1(x)\n",
    "out2_first = drop_2(x)\n",
    "out2_second = drop_2(x)\n",
    "is_equal = torch.eq(out1_first,  out1_second).view((-1,))\n",
    "is_equal_different_layers = torch.eq(out1_first,  out2_first).view((-1,))\n",
    "\n",
    "print(torch.sum(is_equal)/is_equal.numel())\n",
    "print(torch.sum(is_equal_different_layers)/is_equal.numel())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
